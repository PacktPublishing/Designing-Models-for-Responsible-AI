{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQM5JLmV7SxN"
   },
   "source": [
    "# Implementing Differential Privacy with TensorFlow Privacy on IOT Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENIwMUvC7oC6"
   },
   "source": [
    "# Overview of DP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RklQ41M_7rZO"
   },
   "source": [
    "**Differential privacy** (DP) is a framework for **measuring the privacy guarantees** provided by an algorithm. Through the lens of differential privacy, we can design machine learning algorithms that responsibly train models on private data. Learning with differential privacy provides provable guarantees of privacy, mitigating the risk of exposing sensitive training data in machine learning. Intuitively, a model trained with differential privacy should not be affected by any single training example, or small set of training examples, in its data set. This mitigates the risk of exposing sensitive training data in ML.\n",
    "\n",
    "The basic idea of this approach, called differentially private stochastic gradient descent (DP-SGD), is to modify the gradients used in stochastic gradient descent (SGD), which lies at the core of almost all deep learning algorithms. Models trained with DP-SGD provide provable differential privacy guarantees for their input data. There are two modifications made to the vanilla SGD algorithm:\n",
    "\n",
    "\n",
    "\n",
    "1.   First, the sensitivity of each gradient needs to be bounded. In other words, we need to limit how much each individual training point sampled in a minibatch can influence gradient computations and the resulting updates applied to model parameters. This can be done by clipping each gradient computed on each training point.\n",
    "2.   Random noise is sampled and added to the clipped gradients to make it statistically impossible to know whether or not a particular data point was included in the training dataset by comparing the updates SGD applies when it operates with or without this particular data point in the training dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujfNDAtV8YLG"
   },
   "source": [
    "**Problem Statement**: We want to implement Differential Privacy in a deep learning algorithm for an IOT dataset. We want to compare the performance of the model without DP and with DP implementation. We will also study how the model performance (Accuracy) varies with changing the DP parameter epsilon. \n",
    "\n",
    "Accurate occupancy detection of an office room from light, temperature, humidity and CO2 measurements using machine learning models.\n",
    "\n",
    "**Dataset Details**: Occupancy Detection Data Set\n",
    "Link to download: https://archive.ics.uci.edu/ml/datasets/Occupancy+Detection+\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "*   Temperature, in Celsius\n",
    "*   Relative Humidity, %\n",
    "*   Light, in Lux\n",
    "*   CO2, in ppm\n",
    "*   Humidity Ratio, Derived quantity from temperature and relative humidity, in kgwater-vapor/kg-air\n",
    "*  Occupancy, 0 or 1, 0 for not occupied, 1 for occupied status\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xBGo77Vu_Erg"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5zk4FLA__ILN"
   },
   "source": [
    "Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "4-3VU-K5sgU9",
    "outputId": "a19c1b37-7851-4a0d-93a0-6c4ba65414b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 1.x\n",
    "except Exception:\n",
    "  pass\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#tf.compat.v1.logging.set_verbosity(tf.logging.ERROR)\n",
    "import itertools\n",
    "from google.colab import drive\n",
    "from google.colab import files\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (12, 10)\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "#drive.mount('/gdrive')\n",
    "gdrive_data_path = 'data/occupancy_data/'\n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfgCh7o88XRT"
   },
   "source": [
    "Import tensorflow privacy library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QW86h1xEskEw",
    "outputId": "79324def-7cef-42b0-f082-b2f0939dc431"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_privacy in /usr/local/lib/python3.6/dist-packages (0.2.2)\n",
      "Requirement already satisfied: scipy>=0.17 in /usr/local/lib/python3.6/dist-packages (from tensorflow_privacy) (1.4.1)\n",
      "Requirement already satisfied: mpmath in /usr/local/lib/python3.6/dist-packages (from tensorflow_privacy) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy>=0.17->tensorflow_privacy) (1.19.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_privacy\n",
    "\n",
    "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n",
    "from tensorflow_privacy.privacy.optimizers.dp_optimizer import DPGradientDescentGaussianOptimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KB4Y6EMM_Znd"
   },
   "source": [
    "##Load and pre-process the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddMoGoX__rvT"
   },
   "source": [
    "Download the datasets from the link provided and uploaded in google drive location. Preprocess the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GViG1VR8s1K7"
   },
   "outputs": [],
   "source": [
    "def load_data(file_name, dataset_type, display_data=False):\n",
    "  df = pd.read_csv(gdrive_data_path + file_name, sep=\",\")\n",
    "  df.drop('date', axis=1, inplace=True)\n",
    "  cols = df.columns[df.columns != 'Occupancy']\n",
    "  df[df.columns] = df[df.columns].apply(pd.to_numeric)\n",
    "  df[['Temperature', 'Light', 'CO2']] = np.round(df[['Temperature', 'Light', 'CO2']],2)\n",
    "  df[['Humidity']] = np.round(df[['Humidity']],4)\n",
    "  df[['HumidityRatio']] = np.round(df[['HumidityRatio']],6)\n",
    "  if display_data:\n",
    "    print(\"#### Preview data ####\")\n",
    "    print(df.head())\n",
    "  x = df[cols]\n",
    "  y = df[['Occupancy']]\n",
    "  print(dataset_type + \" Shape:\", x.shape, y.shape)\n",
    "  return (x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hCKyxr3Ms36X",
    "outputId": "03b685f1-8f61-4faa-8a70-1fb67cfbdfe7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Preview data ####\n",
      "   Temperature  Humidity  Light     CO2  HumidityRatio  Occupancy\n",
      "1        23.18   27.2720  426.0  721.25       0.004793          1\n",
      "2        23.15   27.2675  429.5  714.00       0.004783          1\n",
      "3        23.15   27.2450  426.0  713.50       0.004779          1\n",
      "4        23.15   27.2000  426.0  708.25       0.004772          1\n",
      "5        23.10   27.2000  426.0  704.50       0.004757          1\n",
      "Train Shape: (8143, 5) (8143, 1)\n",
      "#### Preview data ####\n",
      "     Temperature  Humidity   Light     CO2  HumidityRatio  Occupancy\n",
      "140        23.70    26.272  585.20  749.20       0.004764          1\n",
      "141        23.72    26.290  578.40  760.40       0.004773          1\n",
      "142        23.73    26.230  572.67  769.67       0.004765          1\n",
      "143        23.72    26.125  493.75  774.75       0.004744          1\n",
      "144        23.75    26.200  488.60  779.00       0.004767          1\n",
      "Test Shape: (2665, 5) (2665, 1)\n",
      "#### Preview data ####\n",
      "   Temperature  Humidity   Light      CO2  HumidityRatio  Occupancy\n",
      "1        21.76   31.1333  437.33  1029.67       0.005021          1\n",
      "2        21.79   31.0000  437.33  1000.00       0.005009          1\n",
      "3        21.77   31.1225  434.00  1003.75       0.005022          1\n",
      "4        21.77   31.1225  439.00  1009.50       0.005022          1\n",
      "5        21.79   31.1333  437.33  1005.67       0.005030          1\n",
      "Test Shape: (9752, 5) (9752, 1)\n",
      "(8143, 5) (2665, 5) (9752, 5) (8143, 1) (2665, 1) (9752, 1)\n"
     ]
    }
   ],
   "source": [
    "train = load_data('datatraining.txt', 'Train', True)\n",
    "val = load_data('datatest.txt', 'Test', True)\n",
    "test = load_data('datatest2.txt', 'Test', True)\n",
    "\n",
    "train_data, train_labels = train\n",
    "val_data, val_labels = val\n",
    "test_data, test_labels = test\n",
    "\n",
    "train_data = np.array(train_data, dtype=np.float32)\n",
    "val_data = np.array(val_data, dtype=np.float32)\n",
    "test_data = np.array(test_data, dtype=np.float32)\n",
    "\n",
    "#train_data = train_data.reshape(train_data.shape[0], train_data.shape[1], 1)\n",
    "#test_data = test_data.reshape(test_data.shape[0], test_data.shape[1], 1)\n",
    "\n",
    "train_labels = np.array(train_labels, dtype=np.int32)\n",
    "val_labels = np.array(val_labels, dtype=np.int32)\n",
    "test_labels = np.array(test_labels, dtype=np.int32)\n",
    "\n",
    "print(train_data.shape, val_data.shape, test_data.shape, train_labels.shape, val_labels.shape, test_labels.shape)\n",
    "feature_size = train_data.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RCfq3B-4AMXT"
   },
   "source": [
    "Data Normalisation: The features have been normalised using MinMax Scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gf09CC2MtccA"
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "train_data = scaler.fit_transform(train_data)\n",
    "val_data = scaler.transform(val_data)\n",
    "test_data = scaler.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71X4-Ss-AhO0"
   },
   "source": [
    "##Define and tune learning model hyperparameters\n",
    "Set learning model hyperparamter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3oaQufcG6_IF"
   },
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "batch_size = 32\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Other hyperparameters and functions\n",
    "kernel_regularization = 0.001 #1.2 #0.001352\n",
    "GradientDescentOptimizer = tf.compat.v1.train.GradientDescentOptimizer\n",
    "\n",
    "def reshaper(arr, batch_size):\n",
    "  arr = arr[:-(arr.shape[0] % batch_size),:]\n",
    "  return arr\n",
    "\n",
    "\n",
    "train_data = reshaper(train_data, batch_size)\n",
    "train_labels = reshaper(train_labels, batch_size)\n",
    "val_data = reshaper(val_data, batch_size)\n",
    "val_labels = reshaper(val_labels, batch_size)\n",
    "test_data = reshaper(test_data, batch_size)\n",
    "test_labels = reshaper(test_labels, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VqoLOHjrEn9l"
   },
   "source": [
    "##Build the learning model without DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9M9n-JxntvjF",
    "outputId": "0b72f7c5-3e5c-4193-c887-1a079cb12f1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8128 samples, validate on 2656 samples\n",
      "Epoch 1/30\n",
      "8128/8128 [==============================] - 1s 95us/sample - loss: 0.3731 - acc: 0.9841 - val_loss: 0.3642 - val_acc: 0.9782\n",
      "Epoch 2/30\n",
      "8128/8128 [==============================] - 1s 87us/sample - loss: 0.3509 - acc: 0.9884 - val_loss: 0.3563 - val_acc: 0.9785\n",
      "Epoch 3/30\n",
      "8128/8128 [==============================] - 1s 85us/sample - loss: 0.3454 - acc: 0.9884 - val_loss: 0.3526 - val_acc: 0.9785\n",
      "Epoch 4/30\n",
      "8128/8128 [==============================] - 1s 86us/sample - loss: 0.3425 - acc: 0.9884 - val_loss: 0.3502 - val_acc: 0.9782\n",
      "Epoch 5/30\n",
      "8128/8128 [==============================] - 1s 89us/sample - loss: 0.3404 - acc: 0.9883 - val_loss: 0.3485 - val_acc: 0.9785\n",
      "Epoch 6/30\n",
      "8128/8128 [==============================] - 1s 89us/sample - loss: 0.3388 - acc: 0.9883 - val_loss: 0.3471 - val_acc: 0.9782\n",
      "Epoch 7/30\n",
      "8128/8128 [==============================] - 1s 88us/sample - loss: 0.3375 - acc: 0.9883 - val_loss: 0.3461 - val_acc: 0.9782\n",
      "Epoch 8/30\n",
      "8128/8128 [==============================] - 1s 88us/sample - loss: 0.3365 - acc: 0.9883 - val_loss: 0.3453 - val_acc: 0.9782\n",
      "Epoch 9/30\n",
      "8128/8128 [==============================] - 1s 88us/sample - loss: 0.3357 - acc: 0.9883 - val_loss: 0.3445 - val_acc: 0.9785\n",
      "Epoch 10/30\n",
      "8128/8128 [==============================] - 1s 90us/sample - loss: 0.3350 - acc: 0.9886 - val_loss: 0.3440 - val_acc: 0.9782\n",
      "Epoch 11/30\n",
      "8128/8128 [==============================] - 1s 88us/sample - loss: 0.3345 - acc: 0.9883 - val_loss: 0.3435 - val_acc: 0.9785\n",
      "Epoch 12/30\n",
      "8128/8128 [==============================] - 1s 88us/sample - loss: 0.3340 - acc: 0.9884 - val_loss: 0.3431 - val_acc: 0.9785\n",
      "Epoch 13/30\n",
      "8128/8128 [==============================] - 1s 88us/sample - loss: 0.3335 - acc: 0.9883 - val_loss: 0.3426 - val_acc: 0.9782\n",
      "Epoch 14/30\n",
      "8128/8128 [==============================] - 1s 88us/sample - loss: 0.3330 - acc: 0.9883 - val_loss: 0.3423 - val_acc: 0.9782\n",
      "Epoch 15/30\n",
      "8128/8128 [==============================] - 1s 90us/sample - loss: 0.3327 - acc: 0.9883 - val_loss: 0.3419 - val_acc: 0.9782\n",
      "Epoch 16/30\n",
      "8128/8128 [==============================] - 1s 89us/sample - loss: 0.3323 - acc: 0.9883 - val_loss: 0.3416 - val_acc: 0.9782\n",
      "Epoch 17/30\n",
      "8128/8128 [==============================] - 1s 89us/sample - loss: 0.3320 - acc: 0.9883 - val_loss: 0.3414 - val_acc: 0.9782\n",
      "Epoch 18/30\n",
      "8128/8128 [==============================] - 1s 87us/sample - loss: 0.3317 - acc: 0.9884 - val_loss: 0.3411 - val_acc: 0.9782\n",
      "Epoch 19/30\n",
      "8128/8128 [==============================] - 1s 88us/sample - loss: 0.3314 - acc: 0.9883 - val_loss: 0.3410 - val_acc: 0.9782\n",
      "Epoch 20/30\n",
      "8128/8128 [==============================] - 1s 87us/sample - loss: 0.3312 - acc: 0.9882 - val_loss: 0.3407 - val_acc: 0.9785\n",
      "Epoch 21/30\n",
      "8128/8128 [==============================] - 1s 89us/sample - loss: 0.3310 - acc: 0.9883 - val_loss: 0.3406 - val_acc: 0.9782\n",
      "Epoch 22/30\n",
      "8128/8128 [==============================] - 1s 88us/sample - loss: 0.3309 - acc: 0.9883 - val_loss: 0.3404 - val_acc: 0.9785\n",
      "Epoch 23/30\n",
      "8128/8128 [==============================] - 1s 89us/sample - loss: 0.3307 - acc: 0.9883 - val_loss: 0.3403 - val_acc: 0.9785\n",
      "Epoch 24/30\n",
      "8128/8128 [==============================] - 1s 88us/sample - loss: 0.3305 - acc: 0.9882 - val_loss: 0.3401 - val_acc: 0.9785\n",
      "Epoch 25/30\n",
      "8128/8128 [==============================] - 1s 89us/sample - loss: 0.3304 - acc: 0.9883 - val_loss: 0.3400 - val_acc: 0.9785\n",
      "Epoch 26/30\n",
      "8128/8128 [==============================] - 1s 86us/sample - loss: 0.3302 - acc: 0.9883 - val_loss: 0.3399 - val_acc: 0.9785\n",
      "Epoch 27/30\n",
      "8128/8128 [==============================] - 1s 86us/sample - loss: 0.3301 - acc: 0.9882 - val_loss: 0.3398 - val_acc: 0.9785\n",
      "Epoch 28/30\n",
      "8128/8128 [==============================] - 1s 84us/sample - loss: 0.3300 - acc: 0.9883 - val_loss: 0.3397 - val_acc: 0.9785\n",
      "Epoch 29/30\n",
      "8128/8128 [==============================] - 1s 90us/sample - loss: 0.3299 - acc: 0.9882 - val_loss: 0.3396 - val_acc: 0.9782\n",
      "Epoch 30/30\n",
      "8128/8128 [==============================] - 1s 93us/sample - loss: 0.3299 - acc: 0.9883 - val_loss: 0.3396 - val_acc: 0.9782\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb46028e3c8>"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = tf.keras.Sequential([\n",
    "        tf.keras.Input((feature_size,), name='feature'),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dense(2, activation=tf.nn.softmax, kernel_regularizer=l1(kernel_regularization))\n",
    "    ])\n",
    "\n",
    "optimizer = GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.compat.v2.losses.Reduction.NONE)\n",
    "\n",
    "# Compile model with Keras\n",
    "classifier.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "classifier.fit(train_data,\n",
    "           train_labels,\n",
    "           batch_size=batch_size,\n",
    "           epochs=epochs,\n",
    "           validation_data=(val_data, val_labels),\n",
    "           verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zmed44GrGUH8"
   },
   "source": [
    "##Model Evaluation on test data (without DP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Scf1X3Oitw7d",
    "outputId": "720c92e7-a107-4298-87f4-4340e083d6e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9728/9728 [==============================] - 0s 49us/sample - loss: 0.3315 - acc: 0.9907\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = classifier.evaluate(test_data, test_labels, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMkw1DJAAQAn"
   },
   "source": [
    "##Build the learning model with DP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jhPTpbFLFD99"
   },
   "source": [
    "### Privacy specific Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bez_pIHxE_Cj"
   },
   "source": [
    "DP-SGD has three privacy-specific hyperparameters and one existing hyperamater that you must tune:\n",
    "\n",
    "1. `l2_norm_clip` (float) - The maximum Euclidean (L2) norm of each gradient that is applied to update model parameters. This hyperparameter is used to bound the optimizer's sensitivity to individual training points. \n",
    "2. `noise_multiplier` (float) - The amount of noise sampled and added to gradients during training. **Generally, more noise results in better privacy** (often, but not necessarily, at the expense of lower utility).\n",
    "3.   `microbatches` (int) - Each batch of data is split in smaller units called microbatches. By default, each microbatch should contain a single training example. This allows us to clip gradients on a per-example basis rather than after they have been averaged across the minibatch. This in turn decreases the (negative) effect of clipping on signal found in the gradient and typically maximizes utility. However, computational overhead can be reduced by increasing the size of microbatches to include more than one training examples. The average gradient across these multiple training examples is then clipped. The total number of examples consumed in a batch, i.e., one step of gradient descent, remains the same. **The number of microbatches should evenly divide the batch size. **\n",
    "4. `learning_rate` (float) - This hyperparameter already exists in vanilla SGD. The higher the learning rate, the more each update matters. If the updates are noisy (such as when the additive noise is large compared to the clipping threshold), a low learning rate may help the training procedure converge. \n",
    "\n",
    "Use the hyperparameter values below to obtain a reasonably accurate model (95% test accuracy):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "De9Pnj68E96y"
   },
   "outputs": [],
   "source": [
    "# DP hyperparameters\n",
    "l2_norm_clip = 0.8 #1.0\n",
    "noise_multiplier = 0.8\n",
    "microbatches_perc = .5\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MeIh4sCfAR14",
    "outputId": "decd0c32-20c2-4038-f3c2-3b8704115c94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/clip_ops.py:301: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 8128 samples, validate on 2656 samples\n",
      "Epoch 1/30\n",
      "8128/8128 [==============================] - 5s 639us/sample - loss: 0.5502 - acc: 0.9135 - val_loss: 0.4908 - val_acc: 0.9688\n",
      "Epoch 2/30\n",
      "8128/8128 [==============================] - 5s 611us/sample - loss: 0.4374 - acc: 0.9844 - val_loss: 0.4354 - val_acc: 0.9695\n",
      "Epoch 3/30\n",
      "8128/8128 [==============================] - 5s 615us/sample - loss: 0.4066 - acc: 0.9873 - val_loss: 0.4127 - val_acc: 0.9703\n",
      "Epoch 4/30\n",
      "8128/8128 [==============================] - 5s 618us/sample - loss: 0.3919 - acc: 0.9884 - val_loss: 0.4002 - val_acc: 0.9703\n",
      "Epoch 5/30\n",
      "8128/8128 [==============================] - 5s 626us/sample - loss: 0.3839 - acc: 0.9883 - val_loss: 0.3929 - val_acc: 0.9703\n",
      "Epoch 6/30\n",
      "8128/8128 [==============================] - 5s 620us/sample - loss: 0.3786 - acc: 0.9882 - val_loss: 0.3874 - val_acc: 0.9706\n",
      "Epoch 7/30\n",
      "8128/8128 [==============================] - 5s 618us/sample - loss: 0.3749 - acc: 0.9884 - val_loss: 0.3839 - val_acc: 0.9718\n",
      "Epoch 8/30\n",
      "8128/8128 [==============================] - 5s 626us/sample - loss: 0.3725 - acc: 0.9886 - val_loss: 0.3815 - val_acc: 0.9733\n",
      "Epoch 9/30\n",
      "8128/8128 [==============================] - 5s 617us/sample - loss: 0.3704 - acc: 0.9886 - val_loss: 0.3793 - val_acc: 0.9748\n",
      "Epoch 10/30\n",
      "8128/8128 [==============================] - 5s 617us/sample - loss: 0.3687 - acc: 0.9884 - val_loss: 0.3773 - val_acc: 0.9759\n",
      "Epoch 11/30\n",
      "8128/8128 [==============================] - 5s 614us/sample - loss: 0.3671 - acc: 0.9884 - val_loss: 0.3755 - val_acc: 0.9763\n",
      "Epoch 12/30\n",
      "8128/8128 [==============================] - 5s 621us/sample - loss: 0.3660 - acc: 0.9883 - val_loss: 0.3742 - val_acc: 0.9778\n",
      "Epoch 13/30\n",
      "8128/8128 [==============================] - 5s 621us/sample - loss: 0.3649 - acc: 0.9884 - val_loss: 0.3733 - val_acc: 0.9778\n",
      "Epoch 14/30\n",
      "8128/8128 [==============================] - 5s 620us/sample - loss: 0.3641 - acc: 0.9884 - val_loss: 0.3725 - val_acc: 0.9782\n",
      "Epoch 15/30\n",
      "8128/8128 [==============================] - 5s 625us/sample - loss: 0.3631 - acc: 0.9884 - val_loss: 0.3713 - val_acc: 0.9785\n",
      "Epoch 16/30\n",
      "8128/8128 [==============================] - 5s 623us/sample - loss: 0.3622 - acc: 0.9884 - val_loss: 0.3705 - val_acc: 0.9785\n",
      "Epoch 17/30\n",
      "8128/8128 [==============================] - 5s 617us/sample - loss: 0.3616 - acc: 0.9884 - val_loss: 0.3699 - val_acc: 0.9785\n",
      "Epoch 18/30\n",
      "8128/8128 [==============================] - 5s 632us/sample - loss: 0.3610 - acc: 0.9884 - val_loss: 0.3694 - val_acc: 0.9785\n",
      "Epoch 19/30\n",
      "8128/8128 [==============================] - 5s 618us/sample - loss: 0.3603 - acc: 0.9884 - val_loss: 0.3689 - val_acc: 0.9785\n",
      "Epoch 20/30\n",
      "8128/8128 [==============================] - 5s 625us/sample - loss: 0.3599 - acc: 0.9884 - val_loss: 0.3687 - val_acc: 0.9785\n",
      "Epoch 21/30\n",
      "8128/8128 [==============================] - 5s 612us/sample - loss: 0.3595 - acc: 0.9884 - val_loss: 0.3682 - val_acc: 0.9785\n",
      "Epoch 22/30\n",
      "8128/8128 [==============================] - 5s 615us/sample - loss: 0.3591 - acc: 0.9884 - val_loss: 0.3677 - val_acc: 0.9785\n",
      "Epoch 23/30\n",
      "8128/8128 [==============================] - 5s 630us/sample - loss: 0.3586 - acc: 0.9884 - val_loss: 0.3671 - val_acc: 0.9785\n",
      "Epoch 24/30\n",
      "8128/8128 [==============================] - 5s 619us/sample - loss: 0.3582 - acc: 0.9884 - val_loss: 0.3667 - val_acc: 0.9785\n",
      "Epoch 25/30\n",
      "8128/8128 [==============================] - 5s 616us/sample - loss: 0.3579 - acc: 0.9884 - val_loss: 0.3661 - val_acc: 0.9785\n",
      "Epoch 26/30\n",
      "8128/8128 [==============================] - 5s 615us/sample - loss: 0.3575 - acc: 0.9884 - val_loss: 0.3658 - val_acc: 0.9785\n",
      "Epoch 27/30\n",
      "8128/8128 [==============================] - 5s 613us/sample - loss: 0.3572 - acc: 0.9884 - val_loss: 0.3657 - val_acc: 0.9785\n",
      "Epoch 28/30\n",
      "8128/8128 [==============================] - 5s 630us/sample - loss: 0.3568 - acc: 0.9884 - val_loss: 0.3652 - val_acc: 0.9785\n",
      "Epoch 29/30\n",
      "8128/8128 [==============================] - 5s 618us/sample - loss: 0.3565 - acc: 0.9884 - val_loss: 0.3649 - val_acc: 0.9785\n",
      "Epoch 30/30\n",
      "8128/8128 [==============================] - 5s 632us/sample - loss: 0.3562 - acc: 0.9884 - val_loss: 0.3646 - val_acc: 0.9785\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb4601865f8>"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DP_classifier = tf.keras.Sequential([\n",
    "        tf.keras.Input((feature_size,), name='feature'),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dense(2, activation=tf.nn.softmax, kernel_regularizer=l1(kernel_regularization))\n",
    "    ])\n",
    "\n",
    "optimizer = DPGradientDescentGaussianOptimizer(\n",
    "            l2_norm_clip=l2_norm_clip,\n",
    "            noise_multiplier=noise_multiplier,\n",
    "            num_microbatches=int(microbatches_perc * batch_size),\n",
    "            learning_rate=learning_rate)\n",
    "\n",
    "# Compute vector of per-example loss rather than its mean over a minibatch.\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.compat.v2.losses.Reduction.NONE)\n",
    "\n",
    "DP_classifier.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "DP_classifier.fit(train_data,\n",
    "           train_labels,\n",
    "           batch_size=batch_size,\n",
    "           epochs=epochs,\n",
    "           validation_data=(val_data, val_labels),\n",
    "           verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lxg5HvJrG5gx"
   },
   "source": [
    "## Model Evaluation on test data (With DP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P2uOsrdtAj6s",
    "outputId": "288bc0f7-d1be-4371-ef3c-d8b3c01564ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9728/9728 [==============================] - 0s 49us/sample - loss: 0.3637 - acc: 0.9855\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = DP_classifier.evaluate(test_data, test_labels, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pozMPkfzHNjL"
   },
   "source": [
    "## Measure the differential privacy guarantee\n",
    "\n",
    "Perform a privacy analysis to measure the DP guarantee achieved by a training algorithm. Knowing the level of DP achieved enables the objective comparison of two training runs to determine which of the two is more privacy-preserving. At a high level, the privacy analysis measures how much a potential adversary can improve their guess about properties of any individual training point by observing the outcome of our training procedure (e.g., model updates and parameters). \n",
    "\n",
    "This guarantee is sometimes referred to as the **privacy budget**. A lower privacy budget bounds more tightly an adversary's ability to improve their guess. This ensures a stronger privacy guarantee. Intuitively, this is because it is harder for a single training point to affect the outcome of learning: for instance, the information contained in the training point cannot be memorized by the ML algorithm and the privacy of the individual who contributed this training point to the dataset is preserved.\n",
    "\n",
    "In this tutorial, the privacy analysis is performed in the framework of RÃ©nyi Differential Privacy (RDP), which is a relaxation of pure DP based on [this paper](https://arxiv.org/abs/1702.07476) that is particularly well suited for DP-SGD.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HBuEaa6WHab1"
   },
   "source": [
    "Two metrics are used to express the DP guarantee of an ML algorithm:\n",
    "\n",
    "1.   **Delta** ($\\delta$) - Bounds the probability of the privacy guarantee not holding. A rule of thumb is to set it to be ***less than the inverse of the size of the training dataset***. \n",
    "2.   **Epsilon** ($\\epsilon$) - This is the **privacy budget**. It measures the strength of the privacy guarantee by bounding how much the probability of a particular model output can vary by including (or excluding) a single training point. **A smaller value for $\\epsilon$ implies a better privacy guarantee**. However, the $\\epsilon$ value is only an upper bound and a large value could still mean good privacy in practice.\n",
    "\n",
    "Tensorflow Privacy provides a tool, `compute_dp_sgd_privacy.py`, to compute the value of $\\epsilon$ given a fixed value of $\\delta$ and the following hyperparameters from the training process:\n",
    "\n",
    "1.   The total number of points in the training data, `n`.\n",
    "2. The `batch_size`.\n",
    "3.   The `noise_multiplier`.\n",
    "4. The number of `epochs` of training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "411bKm65BV5a",
    "outputId": "d5fb6d13-ae34-40aa-855d-9f576d42267c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DP-SGD with sampling rate = 0.394% and noise_multiplier = 0.8 iterated over 2540 steps satisfies differential privacy with eps = 2.87 and delta = 1e-05.\n",
      "The optimal RDP order is 6.0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2.874072132038394, 6.0)"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_dp_sgd_privacy.compute_dp_sgd_privacy(n=train_data.shape[0], batch_size=batch_size, noise_multiplier=noise_multiplier, epochs=10, delta=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lmKQ__kqFo9T"
   },
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7TuRSK7Bs4Io"
   },
   "source": [
    "Initially, we trained 108 models to study the variation of different hyperparameters with Test set accuracy and epsilon to coarsely identify the parameters to choose from.\n",
    "\n",
    "Then in order to see the variation of epsilon with delta we kept all other hyper parameters constant and varied delta for wide range of values. \n",
    "\n",
    "Similarly, we studied the variation of other hyper parameters with epsilon.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z5va30hxaS9g"
   },
   "outputs": [],
   "source": [
    "epochs=30\n",
    "def create_model(l2_norm_clip=0.0, noise_multiplier=0.0, microbatches_perc=0.5, learning_rate=0.1, delta=1e-4, kernel_regularization=0.001):\n",
    "\t# create model\n",
    "  model = tf.keras.Sequential([\n",
    "        tf.keras.Input((feature_size,), name='feature'),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dense(2, activation=tf.nn.softmax, kernel_regularizer=l1(kernel_regularization))\n",
    "    ])\n",
    "  optimizer = DPGradientDescentGaussianOptimizer(\n",
    "            l2_norm_clip=l2_norm_clip,\n",
    "            noise_multiplier=noise_multiplier,\n",
    "            num_microbatches=int(microbatches_perc * batch_size),\n",
    "            learning_rate=learning_rate)\n",
    "\n",
    "  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.compat.v2.losses.Reduction.NONE)\n",
    "\n",
    "  model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "  # fit the model\n",
    "  res = model.fit(train_data,\n",
    "          train_labels,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(val_data, val_labels),\n",
    "          verbose=0)\n",
    "  \n",
    "  # look at the last 10 epochs. Get the mean and standard deviation of the validation score.\n",
    "  last10_scores = np.array(res.history['val_acc'][-10:])\n",
    "  mean_val_acc = last10_scores.mean()\n",
    "  std_val_acc = last10_scores.std()\n",
    "\n",
    "  # If the model didn't converge then set a high loss.\n",
    "  if np.isnan(mean_val_acc):\n",
    "      return 9999.0, 0.0\n",
    "\n",
    "  #Evaluate test data performance\n",
    "  test_loss, test_accuracy = model.evaluate(test_data, test_labels, batch_size=batch_size)\n",
    "\n",
    "  #Compute epsilon\n",
    "  dp_res = compute_dp_sgd_privacy.compute_dp_sgd_privacy(n=train_data.shape[0], batch_size=batch_size, noise_multiplier=noise_multiplier, epochs=epochs, delta=delta)\n",
    "  epsilon = dp_res[0]\n",
    "  return mean_val_acc, std_val_acc, test_accuracy, epsilon\n",
    "\n",
    "\n",
    "def model_tuning():\n",
    "  #l2_norm_clip = [1.5]\n",
    "  l2_norm_clip = [3.0, 2.5, 2.0, 1.5, 1.4, 1.3, 1.2, 1.1, 1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0.01, 0.025, 0.05, 0.075, 0.001, 0.0025, 0.005]\n",
    "  noise_multiplier = [0.8]\n",
    "  #noise_multiplier = [5.0, 4.5, 4.0, 3.5, 3.0, 2.5, 2.0, 1.5, 1.4, 1.3, 1.2, 1.1, 1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0.01, 0.025, 0.05, 0.075, 0.001, 0.0025, 0.005, 0.0001, 0.0005]\n",
    "  microbatches_perc = [0.5]\n",
    "  learning_rate = [0.1]\n",
    "  delta = [0.8]\n",
    "  kernel_regularization = [0.001352]\n",
    "\n",
    "  parameters = [params for params in itertools.product(l2_norm_clip, noise_multiplier, microbatches_perc, learning_rate, delta, kernel_regularization)]\n",
    "\n",
    "  for i,param in enumerate(parameters):\n",
    "    total_models = len(parameters)\n",
    "    l2_norm_clip = param[0]\n",
    "    noise_multiplier = param[1]\n",
    "    microbatches_perc = param[2]\n",
    "    learning_rate = param[3]\n",
    "    delta = param[4]\n",
    "    kernel_regularization = param[5]\n",
    "    print(\"Model tuning in progress --- \", i+1, \"/\", total_models)\n",
    "    print(l2_norm_clip, noise_multiplier, learning_rate, delta, kernel_regularization)\n",
    "    mean_val_acc, std_val_acc, test_accuracy, epsilon = create_model(l2_norm_clip, noise_multiplier, microbatches_perc, learning_rate, delta, kernel_regularization)\n",
    "    \n",
    "    col_names = ['experiment_no', 'epochs', 'batch_size', 'l2_norm_clip', 'noise_multiplier', 'microbatches_perc', 'learning_rate', 'delta', 'kernel_regularization', \\\n",
    "                'mean_val_acc', 'std_val_acc', 'test_accuracy', 'epsilon']\n",
    "    res_data = [[i+1,epochs, batch_size, l2_norm_clip, noise_multiplier, microbatches_perc, learning_rate, delta, kernel_regularization, mean_val_acc, std_val_acc,\\\n",
    "                test_accuracy, epsilon]]\n",
    "    res = pd.DataFrame(data = res_data, columns = col_names)\n",
    "    if i == 0:\n",
    "      results = res.copy()\n",
    "    else:\n",
    "      results = pd.concat([results, res], axis=0)\n",
    "    results.reset_index(inplace=True, drop=True)\n",
    "  return results\n",
    "\n",
    "\n",
    "\n",
    "# Training the model\n",
    "results = model_tuning()\n",
    "\n",
    "#Writing results to local machine for later analysis\n",
    "results.sort_values(by='epsilon', axis=0, ascending=True, inplace=True)\n",
    "results.to_csv('DP_results3.csv')\n",
    "files.download('DP_results3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uiEqIeOJVqRP"
   },
   "source": [
    "## Comparison Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZOki1tdOrU1"
   },
   "source": [
    "Delta vs Epsilon Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "eVT5O5YoqzMS",
    "outputId": "ea35ae25-6304-4358-8a14-d59a436e3636"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mpl.rcParams['figure.figsize'] = (6, 6)\n",
    "results = pd.read_csv(gdrive_data_path + 'DP_results1.csv', sep=\",\")\n",
    "fig, ax1 = plt.subplots()\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Delta')\n",
    "ax1.set_ylabel('Epsilon')\n",
    "ax1.plot(results.delta, results.epsilon, color=color)\n",
    "ax1.tick_params(axis='y')\n",
    "plt.xticks(np.arange(1e-7, 1, step=0.1)) \n",
    "plt.title(\"Delta vs Epsilon\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vaqpXz47Fkw2"
   },
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHq3BmvqHTAk"
   },
   "source": [
    "In this experiment, we learned about differential privacy (DP) and how we can implement DP principles in existing ML algorithms to provide privacy guarantees for training data. In particular, we learned how to:\n",
    "\n",
    "* Wrap existing optimizers (e.g., SGD, Adam) into their differentially private counterparts using TensorFlow Privacy\n",
    "* Tune hyperparameters introduced by differentially private machine learning\n",
    "* Measure the privacy guarantee provided using analysis tools included in TensorFlow Privacy"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Implementing DP on IOT dataset.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
